\chapter{Is network replay enough?}
\label{ch:NetworkReplaySurvey}

\section{Overview}
\label{sec:netReplayOverview}

Several existing record and replay have a much higher overhead as they record low level of non-determinism in order to capture and replay the exact state of execution. 
However, most bugs in service oriented application do not require such low level of recording.
We hypothesize that network input with live cloning is enough to trigger most bugs in user-facing services.

To validate this insight, we selected sixteen real-world bugs, applied \parikshan, reproduced them in a production container, and observed whether they were also simultaneously reproduced in the replica.
For each of the sixteen bugs that we triggered in the production environments, \parikshan faithfully reproduced them in the replica. 

We selected our bugs from those examined in previous studies \cite{bugbench,simpleTesting}, focusing on bugs that involved performance, resource-leaks, semantics, concurrency, and configuration. 
We have further categorized these bugs whether they lead to a crash or not, and if they can be deterministically reproduced.
Table \ref{tab:casestudy} presents an overview of our study.

In the rest of this section we discuss the bug-reproduction technique in each of these case-studies in further detail.

\input{parikshan/casestudy-table1}

\section{Applications Targeted}

In our case-studies we have targeted the following applications: MySQL, Apache, Redis, Cassandra, HDFS. Apart from this we have also tried \parikshan on PetStore~\cite{petstore} a J2EE JBOSS~\cite{jboss} multi-tier application. We also did a survey of 217 real-world bugs, and found them similar to the bugs presented in this case study( more details regarding the survey can be found at section~\ref{sec:parikshanSurvey}).
In this section we explain the applications that have been used in the bug case-studies.

\subsection{MySQL}

MySQL~\cite{mysql} is a well known open-source database application for structured SQL queries. 
MySQL is most commonly deployed as a standalone centralized server, and can also be deployed as a cluster service with several servers sharing the data. 
It allows for atomic updates with strict consistency models, so there is a single point of query, and is usually queried using customized \texttt{MYSQL protocol}, which is avialable in several mysql libraries or clients in different languages.
Several modern websites and transaction systems are built on MySQL.
Other softwares which are very similar in deployment to MySQL are Oracle DB~\cite{oracle}, and PostgrepSQL~\cite{postgresql}.

In our examples we have used the native mysql client application provided with the mysql community server distribution.
When using mysql, you can either use an anonymous user, a registered user or an admin.
We have used the default mysql/mysql user or anonymous user to run our queries.

\subsection{Apache HTTPD Server}

Apache httpd server~\cite{apache} is the most well known webservers with millions of active websites being hosted on it.
It responds to \texttt{HTTTP} requests from user-facing browsers, and sends responses from downstream applications to be rendered back in the browser.
Webservers can run standalone, multi-tier in a load-balanced fashion or can act as proxies for security purposes.
Other softwares which are similar to apache are nginx~\cite{nginx}, and thttpd~\cite{thttpd} amongst many others.

Connections to Apache HTTPD servers are made through browsers. For our testing, we have typically used wget~\cite{wget} or httpref~\cite{httpref} command line utilities to run single or multiple workloads of http queries.

\xxx{proxy can be made for HTTP? just add as it's possible}

\subsection{Redis}

Redis~\cite{redis} is an open-source, in-memory, networked key-value storage service. 
The name Redis means Remote Dictionary Server. 
Redis is often ranked the most popular key-value database, and has also been ranked the \#4 NoSQL database in user satisfaction and market presense based on it's reviews. 
It is very light-weight and is commonly used in containers.
Redis maps keys to types of values, and can support many abstract data types apart from strings (e.g. lists, sets, hash tables etc.).
It is also often used as a queuing system.
Other similar softwares include BerkleyDB~\cite{berkleyDB}, and memcached~\cite{memcached}

Redis provides bindings for several languages, and has several client libraries available. 
For our experiments we have used the \emph{redis-cli} client given with the default distribution.

\subsection{Cassandra}

Cassandra~\cite{cassandra} is a well known wide column store NoSQL database, designed to handle large amounts of data across many commodity servers, providing high availability with no single point of failure.
Cassandra offers robust support for clusters spanning multiple datacenters, with asynchronous masterless replication allowing low latency operations for all clients.

%To facilitate re-creating of cassandra bug, we have provided dockerized containers, which 
Installing the correct version of cassandra which has this bug as well as it's dependencies is a little tricky. The re-compilation of the version which has the bug is significantly difficult as some of the dependency libraries are no longer available using simply their Apache IVY build files. We provide these libraries as a part of our package in github project. We also provide a python script for the cassandra client to trigger the bug conditions.

\subsection{HDFS}

The Hadoop distributed file system (HDFS)~\cite{hdfs} is a distributed, scalable, and portable file system written in Java for the Hadoop framework.
A Hadoop cluster has nominally a single namenode plus a cluster of datanodes, although redundancy options are available for the namenode due to its criticality. 
Each datanode serves up blocks of data over the network using a block protocol specific to HDFS. The file system uses TCP/IP sockets for communication. Clients use remote procedure call (RPC) to communicate between each other.
HDFS stores large files (typically in the range of gigabytes to terabytes[64]) across multiple machines. It achieves reliability by replicating the data across multiple hosts.


In our implementation we use HDFS binary client which comes pre-packaged with Hadoop.
We deployed a two node cluster with one master and and two slaves.
The master as primary name-node, and one of the slaves as the secondary name-node.

\section{Case Studies}
\label{sec:parikshanCasestudy}

\xxx{How did we select these bugs? Randomly? How did we try to reproduce them?}

\xxx{This part that was here and I commented out makes no sense in this section - maybe worthwhile moving it to somewhere else to explain *why* someone would use parikshan. but here, we are talking about our core assumption, that network replay is sufficient. This section should be about the study we did to show that this insight is grounded.}


\subsection{Semantic Bugs}
The majority of the bugs found in production SOA systems can be categorized as semantic bugs.
These bugs often happen because an edge condition was not checked during the development stage or there was a logical error in the algorithm etc.
Many such errors result in an unexpected output or possibly can crash the system.
We recreated 4 real-world production bugs from Redis~\cite{redis} queuing system, and Cassandra~\cite{cassandra} a NoSQL database.

\xxx{TODO:Nipun - Add some explanation for Redis, and Cassandra}

\subsubsection{Redis \#761}

In this subsection we describe the Redis\#761 semantic bug \\

\noindent \textbf{Cause of the error:}\\


\noindent The Redis\#761 is an integer overflow error. 
This error is triggered, when the client tries to insert and store a very large number. 
This leads to an unmanaged exception, which crashes the production system. 
Integer overflow, is a common error in several applications. 
We classify this as a semantic bug, which could have been fixed with an error checking condition for large integers. \\


\noindent \textbf{Steps for reproduction:}\\

\begin{adjustwidth}{0.5cm}{}
\begin{enumerate}
	\item Start a redis service with log level set to verbose. Setting loglevel to verbose ensures that we can view whatever is going on inside the service. This is our \productioncontainer.
	\item Create a live clone of the service mapped to a parallel \debugcontainer which will be used to visualize debugging
	\item Start cloning the incoming traffic to both the \productioncontainer and the \debugcontainer asynchronously using \parikshan's network duplicator
	\item Send the following request through the redis client: 
	
		\begin{lstlisting}[language=sh]
		zinterstore out 9223372036854775807 zset zset2
		\end{lstlisting}
		
		This tries to set the variable to the integer in the request, and leads to an integer overflow error. The integer overflow error is simultaneously triggered both in the production and the debug containers.
	
\end{enumerate}
\end{adjustwidth}

\subsubsection{Redis \#487}

In this subsection we describe the Redis\#487 semantic bug \\

\noindent \textbf{Cause of the error:} \\

Redis\#487 resulted in expired keys still being retained in Redis, because of an unchecked edge condition.
While this error does not lead to any exception or any error report in application logs, it gives the user a wrong output.
In the case of such logical errors, the application keeps processing, but the internal state can stay incorrect.
The bug impacts only clients who set keys, and then expire them.\\

\noindent \textbf{Steps for reproduction:} \\

\begin{adjustwidth}{0.5cm}{}
	\begin{enumerate}
			\item Start a redis service with log level set to verbose. Setting loglevel to verbose ensures that we can view whatever is going on inside the service. This is our \productioncontainer.
			\item Create a live clone of the service mapped to a parallel \debugcontainer which will be used to visualize debugging
			\item Start cloning the incoming traffic to both the \productioncontainer and the \debugcontainer asynchronously using \parikshan's network duplicator
			\item flush all keys using \texttt{flushall} command 
			\item set multiple keys using \texttt{set key value} command 
			\item expire a key from one of them using \texttt{expire s 5} command
			\item run the \texttt{keys *} command. This will list keys which should have expired
	\end{enumerate}

\end{adjustwidth}


At the end it can be seen that the expired keys can be listed and accessed in both the production and debug container. This is a persistent error, which does not impact most other aspects of the service. The debug container can be used to look at the transactional logs, and have further instrumentation to understand the root cause of the error.

\subsubsection{Cassandra \#5225}

In this subsection we describe the Cassandra\#5225 semantic bug \\

\noindent \textbf{Cause of the error:} \\

Missing columns when requesting specific columns from a wide row. 
The data is still in the table, just it might not be returned to the user. 
Taking closer look, Cassandra is reading from the wrong column index. 
A problem was found with the index checking algorithm. 
In fact, it was written in reverse.\\

\noindent \textbf{Steps for reproduction:} \\

\begin{adjustwidth}{0.5cm}{}
	\begin{enumerate}
		\item Start a cassandra service in the production container
		\item Use \parikshan's live cloning facility to create a clone of cassandra in the debug-container.
		\item Connect to cassandra using a python client
		\item Insert a large number of columns into cassandra (so that it is a wide row). For our testing we used \texttt{pycassa} python cassandra client. The following code shows column insertion.
		
		\begin{lstlisting}[language=python]
		if need_to_populate:
			print "inserting ..."
			for i in range(100000):
				cols = dict((str(i * 100 + j), 'value%d' % (i * 100 + j)) for j in range(100))
				CF1.insert('key', cols)
				if i % 1000 == 0:
					print "%d%%" % int(100 * float(i) / 100000)
		\end{lstlisting}
		
		\item Fetch the columns in a portion of ranges.The following is an example code
		
		\begin{lstlisting}[language=python]
		for i in range(1000):
			cols = [str(randint(0, 99999)) for i in range(3)]
			expected = len ( set( cols ) )
			results = CF1.get('key', cols)
			if len(results) != expected:
				print "Requested %s, only got %s" % (cols, results.keys())
		\end{lstlisting}
		
		\item At the end of this test case you can observe that some columns were dropped in the response to the client.
	\end{enumerate}

\end{adjustwidth}

\subsubsection{Cassandra \#1837}

In this subsection we describe the Cassandra\#1837 semantic bug \\

\noindent \textbf{Cause of the error:} \\

The main symptom of this error was that deleted columns become available again after doing a flush.
With some domain knowledge, a developer found the error. 
This happens because of a bug in the way deleted rows are interpreted once they leave the memtable. 
In the CFS.getRangeSlice code, flush operation does not delete the data. 
Thus querying for the data shows content as even after deletion.

\noindent \textbf{Steps for reproduction:} \\

\begin{adjustwidth}{0.5cm}{}
	\begin{enumerate}
		\item Start a cassandra service in the production container
		\item Use \parikshan's live cloning facility to create a clone of cassandra in the debug-container.
		\item Using cassandra's command line client, insert columns into cassandra without flushing
		\item delete the inserted column
		\item flush the columns so that the deletion should be committed
		\item query for the columns in the table
		\item observe that the columns have not been deleted and are retained.
	\end{enumerate}
\end{adjustwidth}

Once again we have provide dockerized containers for Cassandra, as well as execution scripts for the client.

\subsection{Performance Bugs}

These bugs do not lead to crashes but cause significant impact to user satisfaction.
A casestudy~\cite{shanluPerf} showed that a large percentage of real-world performance bugs can be attributed to uncoordinated functions, executing functions that can be skipped, and inefficient synchronization among threads (for example locks held for too long etc.).
Typically, such bugs can be caught by function level execution tracing and tracking the time taken in each execution function.
Another key insight provided in~\cite{shanluPerf} was that two-thirds of the bugs manifested themselves when special input conditions were met, or execution was done at scale. 
Hence, it is difficult to capture these bugs with traditional offline white-box testing mechanisms.


\subsubsection{MySQL \#26527}

In this subsection we describe the MySQL\#26527 performance bug \\

\noindent \textbf{Cause of the error:} \\

This has to do with a caching problem for large inserts where large amount of data in a partitioned table.
It was reported that inserting data with \texttt{LOAD DATA INFILE} is very slow with partitioned table and sometimes crawls to a stop.
The reason behind the error was found to be that MySQL uses a handler function to prepare caches for large inserts.
As high availability partitioner didn't allow these caches for underlaying tables, the inserts were much slower.

\noindent \textbf{Steps for reproduction:} \\

\begin{adjustwidth}{0.5cm}{}
	\begin{enumerate}
		\item Start an instance of the MySQL server in the production container
		\item Using \parikshan's live cloning capability create a clone of the \productioncontainer. This is our \debugcontainer
		\item Start network duplicator to duplicate network traffic to both the production and debug containers
		\item connect to the production container using \texttt{mysqlclient}
		\item using \emph{mysql-client} run the following query
		
		\begin{lstlisting}
		CREATE TABLE t1 (
		f1 int(10) unsigned NOT NULL DEFAULT '0',
		f2 int(10) unsigned DEFAULT NULL,
		f3 char(33) CHARACTER SET latin1 NOT NULL DEFAULT '',
		f4 char(15) DEFAULT NULL,
		f5 datetime NOT NULL DEFAULT '0000-00-00 00:00:00',
		f6 char(40) CHARACTER SET latin1 DEFAULT NULL,
		f7 text CHARACTER SET latin1,
		KEY f1_idx (f1),
		KEY f5_idx (f5)
		) ENGINE=MyISAM DEFAULT CHARSET=utf8;
		
		\end{lstlisting}
		
		\item Inserting 64 GB of data takes more than 1 day with this setup. This can be observed with both production and debug containers running in sync, hence the slow performance can be also monitored in the debug container.
		
	\end{enumerate}
\end{adjustwidth}	


\subsubsection{MySQL \#49491}

In this subsection we describe the MySQL\#49491 performance bug \\

\noindent \textbf{Cause of the error:} \\

It was reported that the calculation of MD5 and SHA1 hash values using the built-in MySQL functions does not seem to be as efficient, and takes too long.There seem to be two factors that determine the performance of the hash generation:
\begin{itemize}
	\item computation of the actual hash value (binary value)
	\item conversion of the binary value into a string field
\end{itemize}

The run time of the hash computation depends on the length of the input string whereas the overhead of the binary-to-string conversion can be considered as a fixed constant as it will always operate on hash values of 16 (MD5) or 20 (SHA1) bytes length.
The impact of the binary-to-string conversion will become more visible with shorter input strings than with long input strings. For short input strings it seems that more time is spent in the binary-to-string conversion than in the actual hash computation part.\footnote{A patch provided by a developer improved the performance by an order of magnitude. However for the purposes of our discussion, we have limited ourselves to bug-recreation}

\noindent \textbf{Steps for reproduction:} \\

\begin{adjustwidth}{0.5cm}{}
	\begin{enumerate}
		\item Start an instance of the MySQL server in the production container
		\item Using \parikshan's live cloning capability create a clone of the \productioncontainer. This is our \debugcontainer
		\item Start network duplicator to duplicate network traffic to both the production and debug containers
		\item connect to the production container using \texttt{mysqlclient}
		\item Run a select query from the client on the users database:
		
		\begin{lstlisting}
		select count(\*) from (select md5(firstname) from users) sub limit 1G
		\end{lstlisting}
		
		\item The time observed for this query is reported as a performance bug by the reporter. This can be viewed both in the production container and the debug container
		
	\end{enumerate}
\end{adjustwidth}	



\subsubsection{MySQL \#15811}
\label{sec:mySQL15811}

In this subsection we describe the MySQL\#15811 performance bug \\

\noindent \textbf{Cause of the error:} \\

For one of the bugs in  MySQL\#15811, it was reported that some of the user requests which were dealing with complex scripts (Chinese, Japanese), were running significantly slower than others.
To evaluate \parikshan, we re-created a two-tier client-server setup with the server (container) running a buggy MySQL server and sent queries to the production container with complex scripts (Chinese).
These queries were asynchronously replicated, in the debug container. To further investigate the bug-diagnosis process, we also turned on execution tracing in the debug container using SystemTap~\cite{systemtap}.
This gives us the added advantage, of being able to profile and identify the functions responsible for the slow-down, without the tracing having any impact on production.\\

\noindent \textbf{Steps for reproduction:} \\

\begin{adjustwidth}{0.5cm}{}
	\begin{enumerate}
		\item Start an instance of the MySQL server in the production container
		\item Using \parikshan's live cloning capability create a clone of the \productioncontainer. This is our \debugcontainer
		\item Start network duplicator to duplicate network traffic to both the production and debug containers
		\item connect to the production container using \texttt{mysqlclient}
		\item Create a table with default charset as latin1:
			
			\begin{lstlisting}
			create table t1(c1 char(10)) engine=myisam default charset=latin1;
			\end{lstlisting}
			
		\item Repeat the following line several times to generate a large dataset
		
			\begin{lstlisting}
			insert into t1 select * from t1;
			\end{lstlisting}
	
		\item Now create a mysqldump of the table
		\item Load this table back again, and observe a significant slow response for large table insert requests. This is magnified several times when using complex scripts
	\end{enumerate}
\end{adjustwidth}	



\subsection{Resource Leaks}
Resource leaks can be either memory leak or un-necessary zombie processes.
Memory leaks are common errors in service-oriented systems, especially in C/C++ based applications which allow low-level memory management by users.
These leaks build up over time and can cause slowdowns because of resource shortage, or crash the system.
Debugging leaks can be done either using systematic debugging tools like Valgrind, which use shadow memory to track all objects, or memory profiling tools like VisualVM, mTrace, or PIN, which track allocations, de-allocations, and heap size.
Although Valgrind is more complete, it has a very high overhead and needs to capture the execution from the beginning to the end (i.e., needs application restart).
On the other hand, profiling tools are much lighter and can be dynamically patched to a running process.



\subsubsection{Redis \#417}

In this subsection we describe the Redis\#417 performance bug \\

\noindent \textbf{Cause of the error:} \\

It was reported that when two or more databases are replicated, and atleast one of them is >=db10. This is because the replication feed for the slaves creates static connection objects which are allocated and freed when replication is being done from db0-9. However, for database ID values greater than 10, the objects are dynamically created an never freed.
This was leaving stale memory and leading to a memory leak in redis.
Although the bug is visible in the verbose logs, it is difficult to pick out the root-cause of the bug.

\noindent \textbf{Steps for reproduction:} \\

\begin{adjustwidth}{0.5cm}{}
	\begin{enumerate}
		\item Start a redis service with log level set to verbose. Setting loglevel to verbose ensures that we can view whatever is going on inside the service. This is our \productioncontainer.
		\item Also start a slave along with the master for a two node redis deployment
		\item Create a live clone of the service mapped to a parallel \debugcontainer which will be used to visualize debugging
		\item Start cloning the incoming traffic to both the \productioncontainer and the \debugcontainer asynchronously using \parikshan's network duplicator
		
		\item Execute the following commands concurrently from the redis-client
		
		\begin{lstlisting}[language=sh]
			redis-cli -r 1000000 set foo bar
			redis-cli -n 15 -r 1000000 set foo bar
		\end{lstlisting}
		
		\item After the two sets, check the master \debugcontainer, you can observe the tremendous increase of memory usage - this shows the memory leak.
		
	\end{enumerate}
\end{adjustwidth}	


\subsubsection{Redis \#614}

In this subsection we describe the Redis\#614 performance bug \\

\noindent \textbf{Cause of the error:} \\

The debugger reported replication bug while attempting to use Redis as a reliable queue with a Lua script pushing multiple elements onto the queue. 
It appears the wrong number of RPOP operations are sent to the slave instance, resulting in the queue on the slave growing unbounded, out of sync with master.

\noindent \textbf{Steps for reproduction:} \\

\begin{adjustwidth}{0.5cm}{}
	\begin{enumerate}
		\item Start a redis service with log level set to verbose. Setting loglevel to verbose ensures that we can view whatever is going on inside the service. This is our \productioncontainer.
		\item Also start a slave along with the master for a two node redis deployment
		\item Create a live clone of the service mapped to a parallel \debugcontainer which will be used to visualize debugging
		\item Start cloning the incoming traffic to both the \productioncontainer and the \debugcontainer asynchronously using \parikshan's network duplicator
		
		\item Send data via the producer, the following ruby code is an example
		
		\begin{lstlisting}[language=ruby]
		def run
		   puts "Starting producer"
		   loop do
		        puts "Inserting 4 elements..."
		        @redis.eval(@script, :keys => ['queue'])
	           sleep 1
		   end
		 end
		\end{lstlisting}
		
		\item Data is consumed via a consumer, the following ruby code is an example
		
		\begin{lstlisting}[language=ruby]
		def run
			puts "Starting consumer #{@worker_id}"
			@redis.del(@worker_id)
			loop do
				element = @redis.brpoplpush('queue', @worker_id)
				puts "Got element: #{element}"
				@redis.lrem(@worker_id, 0, element)
			end
		end		
		\end{lstlisting}
		
		\item The verbose log in both the \productioncontainer and the \debugcontainer shows an increasing memory footprint
		
		\begin{lstlisting}
		[3672] 01 Sep 21:39:59.596 - 1 clients connected (0 slaves), 557152 bytes in use
		[3672] 01 Sep 21:40:04.642 - DB 8: 1 keys (0 volatile) in 4 slots HT.
		[3672] 01 Sep 21:40:04.642 - 1 clients connected (0 slaves), 557312 bytes in use
		[3672] 01 Sep 21:40:09.687 - DB 8: 1 keys (0 volatile) in 4 slots HT.
		[3672] 01 Sep 21:40:09.687 - 1 clients connected (0 slaves), 557472 bytes in use
		\end{lstlisting}
		
	\end{enumerate}
\end{adjustwidth}



\subsection{Concurrency Bugs}
One of the most subtle bugs in production systems are caused due to concurrency errors.
These bugs are hard to reproduce, as they are non-deterministic, and may or may not happen in a given execution.
Unfortunately, \parikshan cannot guarantee that if a buggy execution is triggered in the production container, an identical execution will trigger the same error in the debug container.
However, given that the debug container is a live-clone of the production container, and that it replicates the state of the production container entirely, we believe that the chances of the bug also being triggered in the debug container are quite high.
Additionally, the debug container is a useful tracing utility to track thread lock and unlock sequences, to get an idea of the concurrency bug.
The bugs here are taken from the bugbench database~\cite{bugbench}

\subsubsection{Apache \#25520}

In this subsection we describe the Apache\#25520 concurrency bug \\

\noindent \textbf{Cause of the error:} \\

It was reported that when logs are configured to have buffering turned on, the log lines show up as corrupted, when serving at a very high volume using the worker mpm.
The problem appears to be that the per-child buffer management is not thread-safe.
There is nothing to prevent memcopy operations in buffered log writers by different threads from overlapping.
 

\noindent \textbf{Steps for reproduction:} \\

\begin{adjustwidth}{0.5cm}{}
	\begin{enumerate}
		\item Install an httpd service in the production container( install it with configuration \emph{--with-mpm=worker} - this will set apache in multi-thread instead of multi-process mode).
		\item Configure httpd with conf/httpd.conf having:
		
		\begin{lstlisting}
		BufferdLogs on
		\end{lstlisting}
		
		and subsequently start the httpd service in the \productioncontainer
		
		\item Create a live clone of the service mapped to a parallel \debugcontainer which will be used to visualize debugging
		\item Start cloning the incoming traffic to both the \productioncontainer and the \debugcontainer asynchronously using \parikshan's network duplicator
		
		\item Send concurrent requests from the client as follows:
		
		\begin{lstlisting}[language=sh]
		./httperf  --server=<your-httpd-server-name>  --uri=/index.html.cz.iso8859-2 --num-conns=100 --num-calls=100
		./httperf  --server=<your-httpd-server-name>  --uri=/index.html.en --num-conns=100 --num-calls=100
		\end{lstlisting}
		
		\item The bug might manifest in \emph{accesslog}, showing some of the logs to be corrupted. In our experiments, we were able to see the anomaly in both \productioncontainer and \debugcontainer simultaneously most of the time.
		
	\end{enumerate}
\end{adjustwidth}

It should be noted that in this bug, it is not important that the same order should trigger this datarace condition. Simply the presence of a datarace(visible through log corruption) is enough to indicate the error, and can be a starting point for the debugger in the \debugcontainer. This is a bug that is persistent in the system and does not cause a crash.

\subsubsection{Apache \#21287}

In this subsection we describe the Apache\#21287 concurrency bug \\

\noindent \textbf{Cause of the error:} \\

It was reported that there are no mutex lock protection in a reference pointer cleanup operation. 
This leads to an atomicity violation which can cause a dangling pointer and lead to an apache crash.

\noindent \textbf{Steps for reproduction:} \\

\begin{adjustwidth}{0.5cm}{}
	\begin{enumerate}
		\item Install an httpd service in the production container with the following options - (mpm-worker, enabled caching, enabled mem caching)
		\item Configure and install \emph{php} service with your httpd server in the \productioncontainer
		\item Create a live clone of the service mapped to a parallel \debugcontainer which will be used to visualize debugging
		\item Start cloning the incoming traffic to both the \productioncontainer and the \debugcontainer asynchronously using \parikshan's network duplicator
		
		\item Run multiple httperf commands concurrently in the client
		
		\begin{lstlisting}[language=sh]
		   ./httperf --server=<your-httpd-server-name> --uri=/pippo.php?variable=1111 --num-conns=1000 --num-calls=1000
		\end{lstlisting}
		
		\item If the bug manifests, you will observe a crash within 10 seconds of sending the message. In our experiments we were able to observe this bug in multiple execution in both \productioncontainer and \debugcontainer simultaneously.
		
	\end{enumerate}
\end{adjustwidth}


\subsubsection{MySQL \#644}

In this subsection we describe the MySQL\#644 concurrency bug \\

\noindent \textbf{Cause of the error:} \\

This bug is caused by one thread's write-read access pair interleaved by another thread's write access. 
As a result, the read access mistakenly gets an wrong value and leads to program misbehavior.
We used a sqlreplay utility provided in bugbench to recreate this bug. It eventually leads to a system crash

\noindent \textbf{Steps for reproduction:} \\

\begin{adjustwidth}{0.5cm}{}
	\begin{enumerate}
		\item Start an instance of the MySQL server in the production container
		\item Using \parikshan's live cloning capability create a clone of the \productioncontainer. This is our \debugcontainer
		\item Start network duplicator to duplicate network traffic to both the production and debug containers
		\item connect to the production container using \emph{runtran} provided in the bugbench database
		
		\begin{lstlisting}
		./runtran --repeat --seed 65323445 --database test --trace populate_db.txt --monitor pinot --thread 9 --host localhost 30 360 1 results
		\end{lstlisting}
		
		\item If the error happens it leads to a system crash
		
	\end{enumerate}
\end{adjustwidth}	


\subsubsection{MySQL \#169}

In this subsection we describe the MySQL\#169 concurrency bug \\

\noindent \textbf{Cause of the error:} \\
It was reported that the Writing to binlog was not a transactional operation and there was a data-race. 
This leads to binlog showing that operations happen in a different order than how they were actually executed

\noindent \textbf{Steps for reproduction:} \\

\begin{adjustwidth}{0.5cm}{}
	\begin{enumerate}
		\item Start an instance of the MySQL server in the production container
		\item Using \parikshan's live cloning capability create a clone of the \productioncontainer. This is our \debugcontainer
		\item Start network duplicator to duplicate network traffic to both the production and debug containers
		\item connect to the production container using \texttt{mysqlclient}
		
		suppose we have a table named 'b' with schema: (id int) in database 'test'.
		Run the following requests:
		
		\begin{lstlisting}[language=sh]
		./mysql -u root -D test -e 'delete from b' &
		./mysql -u root -D test -e 'insert into b values (1)' &
		\end{lstlisting}
		
		\item Wait and press enter. You will see detection log entry and the insert log entry has disordered binlog index.
		
	\end{enumerate}
\end{adjustwidth}	


\subsubsection{MySQL \#791}

In this subsection we describe the MySQL\#791 performance bug \\

\noindent \textbf{Cause of the error:} \\

This bug is caused by one thread's write-write access pair interleaved by another thread's read access. As a result, the read access mistakenly gets an intermediate value and leads to program misbehavior.

\noindent \textbf{Steps for reproduction:} \\

\begin{adjustwidth}{0.5cm}{}
	\begin{enumerate}
		\item Start an instance of the MySQL server in the production container
		\item Using \parikshan's live cloning capability create a clone of the \productioncontainer. This is our \debugcontainer
		\item Start network duplicator to duplicate network traffic to both the production and debug containers
		\item connect to the production container using \texttt{mysqlclient}
		
		\begin{lstlisting}[language=sh]
		./mysql -u root -e 'flush logs';
		./mysql -u root -D test -e 'insert into atab values(11)';
		\end{lstlisting}
		
		If the bug is triggered you will observe that the insert is NOT recorded in mysql bin log.
		
	\end{enumerate}
\end{adjustwidth}	


\subsection{Configuration Bugs}
Configuration errors are usually caused by wrongly configured parameters, i.e., they are not bugs in the application, but bugs in the input (configuration).
These bugs usually get triggered at scale or for certain edge cases, making them extremely difficult to catch.


\subsubsection{Redis \#957}

In this subsection we describe the Redis\#957 configuration bug \\

\noindent \textbf{Cause of the error:} \\

A user reported an error, which eventually turned out to be misconfiguration error on his part.
The client in Redis is scheduled to be closed ASAP for overcoming of output buffer limits in the masters log file.
Essentially, it happens the DB is configured to be larger than the client-output-buffer-limit.
The connection with the slave times out and it's unable to sync because of the large data.
While the bug is partially a semantic bug, as it could potentially have checks and balances in the code. 
The root cause itself is a lower output buffer limit.
Once again, it can be easily observed in our debug-containers that the slave is not synced, and can be investigated further by the debugger.

\noindent \textbf{Steps for reproduction:} \\

\begin{adjustwidth}{0.5cm}{}
	\begin{enumerate}
		\item Start a redis service with log level set to verbose. Setting loglevel to verbose ensures that we can view whatever is going on inside the service. This is our \productioncontainer.
		\item configure redis using :
		
		\begin{lstlisting}
		client-output-buffer-limit slave 256mb 64mb 60
		\end{lstlisting}
		
		\item Create a live clone of the service mapped to a parallel \debugcontainer which will be used to visualize debugging
		\item Start cloning the incoming traffic to both the \productioncontainer and the \debugcontainer asynchronously using \parikshan's network duplicator
		
		\item Load a very large DB into master
		
		You will observe that the connection to the slave is lost on syncing
		
	\end{enumerate}
\end{adjustwidth}


\subsubsection{HDFS \#1904}

In this subsection we describe the HDFS\#1904 configuration bug \\

\noindent \textbf{Cause of the error:} \\

This is sort of a semantic and configuration bug both. It was reported that HDFS crashes if a mkdir command is given through the client in a non-existent folder.

\noindent \textbf{Steps for reproduction:} \\

\begin{adjustwidth}{0.5cm}{}
	\begin{enumerate}
		\item Install hadoop and configure secondary namenode with fs.checkpoint.period set to a small value (eg 3 seconds)
		\item Format filesystem and start HDFS
		\item Create a live clone of the service mapped to a parallel \debugcontainer which will be used to visualize debugging
		\item Start cloning the incoming traffic to both the \productioncontainer and the \debugcontainer asynchronously using \parikshan's network duplicator
		\item Run the following command through the client
		
		\begin{lstlisting}[language=sh]
		hadoop fs -mkdir /foo/bar; 
		sleep 5 ; 
		echo | hadoop fs -put - /foo/bar/baz
		\end{lstlisting}		
	\end{enumerate}

Secondary Name Node will crash with the following trace on the next checkpoint. The primary NN also crashes on next restart

\begin{lstlisting}
ERROR namenode.SecondaryNameNode: Throwable Exception in doCheckpoint:
ERROR namenode.SecondaryNameNode: java.lang.NullPointerException: Panic: parent does not exist
\end{lstlisting}

\end{adjustwidth}

%\section{Discussion}
%\label{sec:parikshanNetworkSumDiscussion}

\xxx{Network input }

\input{parikshan/bugTriggerEval}

\section{Summary}
\label{sec:parikshanCaseStudySummary}

In this section we presented 16 real-life bug cases from 5 different categories of bugs: Semantic, Performance, Resource Leaks, Concurrency, and Configuration Bugs.
These bugs spanned 5 well known open-source softwares.
For each of these bugs, we presented it's symptoms and how they were re-created using parikshan's network duplication.
Further we also presented a survey of 217 bugs from 3 well known applications, where we manually classify the bugs in these systems to the above given categories.


The bugs discussed in this chapter are a representative of real world bugs for SOA applications.
Based on our results, we believe that network duplication is enough to capture most bugs, and trigger them in \parikshan's \debugcontainer.