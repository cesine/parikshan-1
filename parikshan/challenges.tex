
\section{Discussion and Limitations}
\label{sec:parikshanThreats}

Through our case studies and evaluation, we concluded that \parikshan can faithfully reproduce many real bugs in complex applications with no running-overhead.
However, there may be several threats to the validity of our experiments.
For instance, in our case study, the bugs that we selected to study may not be truly representative of a broad range of different faults.
Perhaps, \parikshan's low-overhead network record and replay approach is less suitable to some classes of bugs.
To alleviate this concern, we selected bugs that represented a wide range of categories of bugs, and further, selected bugs that had already been studied in other literature, to alleviate a risk of selection bias.
We further strengthened this studied with a follow-up categorization of 217 bugs in three real-world applications, finding that most of those bugs were semantic in nature, and very few were non-deterministic, and hence, having similar characteristics to those 16 that we reproduced. \\

\noindent There are also some underlying limitations and assumptions regarding \parikshan's applicability:

\xxx{Clarify what exactly we can and cannot do WRT non-determinism and distirbuted services.}


\subsection{Non-determinism} 
\label{sec:parikshanThreatsNonDeterminism}
Non-determinism can be attributed to three main sources (1) system configuration, (2) application input, and (3) ordering in concurrent threads.
Live cloning of the application state ensures that both applications are in the same ``system-state'' and have the same configuration parameters for itself and all dependencies.
%Furthermore, in offline debugging it is often difficult to capture all possible inputs, and hence deal with input non-determinism.
\parikshan's network proxy ensures that all inputs received in the production container are also forwarded to the debug container.
However, any non-determinism from other sources (e.g. thread interleaving, random numbers, reliance on timing) may limit \parikshan's ability to faithfully reproduce an execution. 
%However, concurrency based non-determinism can still lead to different execution paths in the production and debug containers.
While our current prototype version does not handle these, we believe there are several existing techniques that can be applied to tackle this problem in the context of live debugging.
However, as can be seen in our case-studies above, unless there is significant non-determinism, the bugs will still be triggered in the replica, and can hence be debugged. 
Approaches like statistical debugging~\cite{Liblit:2004:CBI}, can be applied to localize bug.
%Furthermore, techniques like deterministic scheduling~\cite{smt:cacm}, can also be used to counter concurrency based bugs.
\parikshan allows debugger to do significant tracing of synchronization points, which is often required as an input for constraint solvers~\cite{dpor,best}, which can go through all synchronization orderings to find concurrency errors.
We have also tried to alleviate this problem using our divergence checker (Section~\ref{sec:parikshanDivergenceChecking})


\subsection{Distributed Services} 
\label{sec:parikshanThreatsDirstributed}

Large-scale distributed systems are often comprised of several interacting services such as storage, NTP, backup services, controllers and resource managers.
\parikshan can be used on one or more containers and can be used to clone more than one communicating .
Based on the nature of the service, it may be (a). Cloned, (b). Turned off or (c). Allowed without any modification.
For example, storage services supporting a replica need to be cloned or turned off (depending on debugging environment) as they would propagate changes from the debug container to the production containers.
Similarly, services such as NTP service can be allowed to continue without any cloning as they are broadcast based systems and the debug container cannot impact it in anyway.
Furthermore, instrumentation inserted in the replica, will not necessarily slowdown all services.
For instance, instrumentation in a MySQL query handler will not slowdown file-sharing or NTP services running in the same container.

\subsection{Overhead in Parikshan}
\label{sec:parikshanOverhead}

The key motivation of \parikshan is to remove all potential overheads such that instrumentation in the debug-container does not impact performance of the production application.
We wish to clarify certain aspects which may lead to questions regarding overheads in the mind of the reader:
\begin{itemize}
	\item \textbf{Container virtualization:} Based on recent studies, user-space container virtualization give near native performance~\cite{performanceComparisonlxcVM,performanceEvalContainers}. 
	User-space containers essentially leverage process level isolation and do not have a full just-in-time virtualization stack.
	Since several existing SOA applications are deployed in virtualized cloud environments (including full virtualization), we believe that there is no additional overhead from \parikshan as far as container virtualization is concerned
	
	\item \textbf{Network Forwarding:} 
	Another potential source of overhead is network forwarding due to in-memory copy of the data packets being forwarded to the debug-container. 
	To evaluate (see section~\ref{sec:end2endEval}) the overhead we looked at how network overhead can impact bandwidth and latency in both raw TCP requests (micro-benchmarks), as well as how it impacted a few real-world applications (wikibench, and mysql).
	When compared to SOA applications with proxies, we found that the impact in both throughput and latency was negligible (max 1-2\%).
	We also verified \textbf{that increasing the overhead in the debug container has no impact on the production service}. 
	Given that proxies are used commonly in deployed web/service level applications, we could clearly demonstrate that duplication does not add any discernible overhead to production services. 
	Web proxies like squid~\cite{squid} are commonly used to give an order of magnitude performance improvement, and reducing system load by caching frequently fetched pages and links.
	\parikshan can easily be coupled with such already existing web proxies in the system thereby not adding a new network hop by introducing it's own proxy.
	
	\item \textbf{Live Cloning:}
	The reader may also be concerned with overhead due to live cloning.
	Live cloning involves a small time during which the machine must be suspended, this can impact the latency of requests.
	Firstly, it is important to point out that live cloning is \textbf{a single-time process (or periodic)}, and does not impact the general processing of requests in the SOA application, when we are not trying to sync with the production container.   
	The amortized cost of this momentary suspend process process on a live running production application is generally considered acceptable (consider that live migration is used in production systems all the time).
	
	The current implementation for live cloning shown in this thesis is derived from early work in live migration in container virtualization of openvz container virtualization~\cite{vzctl}. 
	We designed this mostly for the \emph{purposes of demonstrating a viable prototype} where live cloning is possible.
	While live migration is a relatively well researched topic in full virtualized systems, it is relatively new in container virtualization.
	Furthermore, network file system support can tremendously improve cloning time and decrease suspension time.
	Live migration is actively used in production systems of several well-known cloud service providers such as amazon~\cite{ec2}, google compute~\cite{gcompute} etc.
	With further advancement in live migration technologies in the user-space container virtualization, state-of-art migration techniques can be modified for live-cloning and can help in the adoption of \parikshan with much shorter suspend times.
	

\end{itemize}

