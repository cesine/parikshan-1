\chapter{Applications of Live Debugging}
\label{ch:activedebugging}

\section{Overview}
\label{sec:guided_overview}

In the sections until now, we have introduced a framework for \livedebugging, a tool for pre-packaging binaries to make them \livedebugging friendly. 
We now discuss some applications of \livedebugging in the real-world, and how it can be used for actual debugging with existing tools or by modifying existing mechanisms.

The debug container allows debuggers to apply any ad-hoc technique used in offline debugging.
However, in order for us to have continuous debugging, it is essential to allow forward progress of the execution in the debug container. 
Furthermore, the divergence due to instrumentation should not stop forward-progress in the debug-container.
For instance, traditional monitoring approaches such as execution traces, memory or performance profiling,  which do not change the state or logic of the executing component can be directly applied to the debug-container with little chance of \debugcontainer diverging. 
The debug container in this case offers the advantage of allowing for a much higher instrumentation overhead compared to what would be generally allowed in production services.
Similarly the \debugcontainer can be used as a staging mechanism for record-replay on demand to ensure deterministic execution.
It is essential however, that none of them functionally modifies the application or else makes any modifications such that forward progress is impossible.
Compared to other approaches with heavier impact like step-by-step execution in interactive debugging tools, or alternatively dynamic instrumentation through tools like Valgrind~\cite{valgrind} or PIN~\cite{pin} which require a controlled debugging environment, \parikshan's debug container is a safe blackbox which allows debugging or monitoring without any impact on production services.

This chapter is divided in the following key parts: Firstly, in section~\ref{sec:guidedDiscussion}, we discuss the advantages and limitations of \parikshan in real-world settings.
We highlight certain key points that the debugger must be aware of when debugging applications with \parikshan, so that he can do reliable and fast debugging. 
In section~\ref{sec:activeDebuggingScenarios}, we classify debugging production applications in two high level debugging scenario categories: \emph{post-facto}, and \emph{proactive} analysis.
We leverage these classifications to explain how different techniques can be applied in \parikshan and the limitations of our system.
Next we list some existing debugging technologies, like statistical debugging, execution tracing, record and replay etc. to explain how they can be either directly applied or modified slightly and applied with the \parikshan framework to significantly improve their analysis.

Lastly, in section~\ref{sec:activeBudgetLimited} we introduce budget-limited adaptive instrumentation. Which focuses further on how to allow for continuous debugging with the maximum information gain.
One of the key criteria for successful statistical debugging is to have higher instrumentation rates, to make the results more statistically significant. 
There is a clear trade-off between instrumentation vs performance overhead for statistical instrumentation. 
A key advantage of using this with \parikshan is that we can provide live feedback based buffer size and bounded overheads, hence squeezing the maximum advantage out of statistical debugging without impacting the overhead. 
We evaluate the slack available in each request for instrumentation without risking a buffer overflow and getting out of sync of the production container.
Budget limited instrumentation is inspired from statistical debugging~\cite{statisticalDebugging}, and focuses on a two-pronged goal of bounding the instrumentation overhead to avoid buffer overflows in \parikshan, and simultaneously have maximum feedback regarding information gained from real-time instrumentation.

\section{Live Debugging using Parikshan}
\label{sec:guidedDiscussion}

The following are some key advantages of \parikshan that can be leveraged for debugging user-facing applications on the fly:

\begin{itemize}
	
	\item \textbf{Sandboxed Environment}: The debug container runs in a sandboxed environment which is running in parallel to the real production system, but any changes in the \debugcontainer are not reflected to the end-user. This is a key advantage in several debugging techniques which are disruptive, and can change the final output. 
	
	Normal debugging mechanisms such as triggering a breakpoints or patching in a new exception/assertion to figure out if a particular ``condition" in the application system state is breaking the code, cannot be done in a production code as it could lead to a critical system crash. On the other hand, \parikshan's debug containers are ideal for this scenario as they will allow developers to put in a patches without any fear of system failure.
	
	\item \textbf{Minimal impact on production system}: The most novel aspect of \parikshan is that it has negligible impact of instrumentation on the production system. 
	This means that high-overhead debugging techniques can be applied on the debug-container incurring a negligible slow-down in production containers.
	
	Debugging techniques like record-replay tools which have traditionally high recording overheads can generally not be applied in production systems. However, \parikshan can be used to decouple the recording overhead from production, and can allow for relatively higher overhead recording with more granularity.
	Section~\ref{sec:end2endEval} discusses evaluation results demonstrating \parikshan's negligible impact on production services.
	
	\item \textbf{Capturing production system state}:
	One of the key factors behind capturing the root-cause of any bug is to capture the system state in which it was triggered. \parikshan has a live-cloning facility that clones the system state and creates a replica of the production. Assuming that the bug was triggered in the production, the replica captures the same state as the production container.
	
	\item \textbf{Compartmentalizing large-scale systems context}:
	Most real-world services are deployed using a combination of several SOA applications, each of them interacting together to provide an end-to-end service. This could be a traditional 3 tier commerce system, with an application layer, a database layer and a web front-end, or a more scaled out social media system with compute services, recommendation engines, short term queuing systems as well as storage and database layers. 
	Bugs in such distributed systems are particularly difficult to re-create as they require the entire large-scale system to be re-created in order to trigger the bug. 
	Traditional record-replay systems if used are insufficient as they are usually focused on a small subset of applications.
	
	Since, our framework leverages network duplication, \parikshan can allow looking at applications in isolation and capturing the system state as well as the input of the running application, without having to re-create the entire buggy infrastructure. 
	In a complex multi-tier system this is a very useful feature to localize the bug.\\
	
	
\end{itemize}

\noindent Above we summarized some of the advantages of using \parikshan, next we look at some of the things an operator should keep in mind when using \parikshan for debugging purposes:

\begin{itemize}
	\item \textbf{Continuous Debugging and Forward Progress}:
	%\livedebugging allows users to debug applications on-the-fly. 
	The debug-container is where one can do debugging runs in parallel to the production container. This is done by first making a live replica of the system followed by duplicating and sending the network input to the \debugcontainer. 
	In a way the \debugcontainer still communicates with the entire system although it's responses are dropped. 
	To ensure forward progress in the \debugcontainer, it is essential that the \debugcontainer is in-sync with the production container, so that the responses, and the requests from the network are the expected responses for forward progress in the application running on the \debugcontainer.
	
	Take for instance, a MySQL~\cite{mysql} service running in the \productioncontainer and \debugcontainer. If during our debugging efforts we modify the state of the debug service such that the MySQL database is no longer in synch with the production service, then any future communication from the network could lead to the state of the debug-container to further diverge from the production. Additionally, depending on the incoming requests or responses the debug application may crash or not have any forward progress.
	
	No forward-progress does not necessarily mean that debugging cannot take place, however for further debugging, once the machine has crashed it needs to be re-cloned from the production container. 
	%Given application knowledge the debugger should avoid state changes that would lead to an crash.
	
	\item \textbf{Debug Window}:
	As explained earlier, most debugging mechanisms generally require instrumentation and tracking execution flow. This means that the application will spend some compute cycles in logging instrumentation points thereby having a slow-down. 
	While \parikshan avoids slow-down in the production environment, there will be some slow-down in the debug-container. 
	
	The amount of time till which the production container remains in synch with the \debugcontainer is called the debug-window(see section~\ref{sec:parikshanWindow}). 
	The window time depends on the overhead, the size of the buffer and the incoming request rate. 
	If a buffer overflow happens because the debug-window has finished, the \debugcontainer needs to be re-synced with the production container.
	
	In our experiments, we have observed, that \parikshan is able to accommodate significant overhead (an order of magnitude depending on workload) without incurring a buffer overflow.	
	Administrators or debuggers using \parikshan should keep the overhead of their instrumentation in mind when debugging in \parikshan. 
	\productioncontainer can always be re-cloned to start a new debugging session.
	
	\item \textbf{Non-determinism}:
	
	One of the most difficult bugs to localize are non-deterministic bugs. While \parikshan is able to capture system non-determinism by capturing the input, it is unable to capture thread non-determinism. 
	Most service-oriented applications have a large number of threads/processes, which means that different threading schedules can happen in the production container as compared to the debug-container.
	This means, that a specific ordering of events that caused a bug to be triggered in the production container, may not happen in the debug-container.
	
	There are multiple ways that this problem can be looked at. 
	Firstly, while it's difficult to quantify, for all the non-deterministic cases in our case-studies, we were able to trigger the bug in both the production and the replica. 
	In the case where the bug is actually triggered in the \debugcontainer, the debugging can take place as usual for other other bugs.
	If that is not the case, there are several techniques which provide systematic ``search"~\cite{pres,best} for different threading schedules based on a high granularity recording of all potential thread synchronization points, and read/write threads. 
	While such high granularity recording is not possible in the production container, it can definitely be done in the \debugcontainer without any impact on the production service.
	
\end{itemize}

\section{Debugging Strategy Categorization}
\label{sec:activeDebuggingScenarios}

Based on our case-studies, and survey of commonly seen SOA bugs we classify the following scenarios for live debugging. 
In each of the scenarios we explain how different categories of bugs can be caught or analyzed.

\subsection{Scenario 1: Post-Facto Analysis}
\label{sec:activePostFactoAnalysis}

In this scenario, the error/fault happens without \livedebugging having been turned on i.e. the service is only running in the production container, and there is no replica.
Typically light-weight instrumentation or monitoring is always turned on in all service/transaction systems. 
Such monitoring systems are very limited in their capabilities to localize the bug, but they can indicate if the system is in a faulty state.

For our post-facto analysis, we use such monitoring systems as a trigger to start \livedebugging once faulty behavior is detected. 
The advantage of such an approach is that debugging resources are only used on-demand, and in an otherwise normal system only the \productioncontainer is utilizing the resources.


There are three kind of bugs that can be considered in this kind of situation:

\begin{itemize}
	
	\item \textbf{Persistent Stateless Bugs}: 
	
	This is the ideal scenario for \parikshan.
	Persistent bugs are those that persist in the application and are long running. 
	They can impact either some or all the requests in a SOA application.
	Common examples of such bugs are memory leak, performance slow-down, semantic bugs among others.
	Assuming they are statistically significant, persistent bugs will be triggered again and again by different requests.
	
	We define \emph{stateless bugs} here as bugs which do not impact the state of the system, hence not impacting future queries. 
	For instance read only operations in the database are stateless, however a write operation which corrupts or modifies the database is stateful, and is likely to impact and cause errors in future transactions.
	
	Hence, such bugs are only dependent on the current system state, and the incoming network input.
	Once such a bug is detected in the production system, \parikshan can initiate a live cloning process and create a replica for debugging purposes. 
	Assuming similar inputs which can trigger the bug are sent by the user, the bug can be observed and debugged the \debugcontainer.
	
	\item \textbf{Persistent Stateful Bugs}:
	
	Stateful bugs are bugs which can impact the system state and change it such that any such bug impacts future transactions in the production container as well.
	For instance in a database service a table may have been corrupted, or it's state changed so that certain transactions are permanently impacted. 
	While having the execution trace of the initial request which triggered a faulty state is useful, the ability to analyze the current state of the application is also extremely useful in localizing the error.
	
	Creating a live clone after such an error and checking the responses state of future impacted transaction, as well as the current state of the database can be a good starting point towards resolving the error. 
	
	\item \textbf{Crashing Bugs}: 
	
	Crashing bugs are bugs that lead to a crash in the system thereby stopping the service.
	Unhandled exceptions, or system traps are generally the cause of such crashes.
	Unfortunately \parikshan has limited utilization for post-facto analysis of a crashing bug. 
	Since \parikshan is not turned ``on" at the time of the crash, any post-facto analysis for creating a \debugcontainer is not possible.

	
\end{itemize} 

\subsection{Scenario 2: Proactive Analysis}
\label{sec:activeProactiveAnalysis}

Proactive analysis is the scenario where the user starts debugging when the system is performing normally and their is no bug. 
This is the same as monitoring a production server, except that in this case the instrumentation is actually present in the \debugcontainer.

Compared to traditional monitoring, one possible use-case is to use the \debugcontainer to do high granularity monitoring at all times. 
This is extremely useful to have if you expect to have higher overheads of instrumentation, which are unacceptable in the production environment. 
Since the \debugcontainer can have much higher instrumentation without any performance penalty on the production container, the instrumentation can be easily put there, and stay active at all times.
Another useful feature is the case where the debugger needs to put in breakpoints or assertions which can cause the system to crash. 
It is not possible to put such assertions, in active systems, but they can be put in \debugcontainer to trigger future analysis.

Proactive recording is basically use to track bugs that could happen in the future as the transaction or request which causes the failure is caught as well, as well as the system state. 
Once a bug is caught, the cause can be independently explored in the \debugcontainer.
It is useful for both stateless and stateful bugs, we do not differentiate between them here as even in the case of a stateful bug, debugging is always turned on. 
Proactive approaches can be compared to existing approaches like statistical debugging~\cite{statisticalDebugging} which use active statistical profiling to compare between successful and buggy runs, to isolate the problem. 
We discuss statistical debugging in section~\ref{sec:activeStatisticalDebugging} and present an advanced approach based on the same in section~\ref{sec:activeBudgetLimited}.
Other proactive body of work include record-replay infrastructures, which record production systems, and can replay the execution if a bug is discovered.
In section~\ref{sec:activeStagedRecordReplay}, we have discussed another variant of proactive debugging called ``staged record-and-replay", which is an advanced record-replay technique that can be applied with the help of \parikshan.



\section{Existing Debugging Mechanisms and Applications}
\label{sec:activeExistingTechniques}

\subsection{Execution Tracing}
\label{sec:activeExecutionTracing}

One of the most common techniques to debug any application is execution tracing. 
Execution tracing gives a trace log of all the functions/modules executed when an input is received. 
This helps the debugger in looking at only those execution points and makes it easier to reason out what is going wrong.

Execution tracing can happen at different granularity: for instance an application can be monitored at function level granularity (only entry and exit of function is monitored), or for deeper understanding at read/write, synchronization point or even instruction level granularity.
Depending on how much granularity the tracing is done at the overhead may be unacceptable for production systems.

\parikshan allows users to de-couple execution tracing from production execution by putting their instrumentation in the \debugcontainer.
As mentioned earlier, this allows for higher level instrumentation at no cost to the production environment. 
%Such techniques which decouple execution and analysis also been previously explored in work like Aftersight~\cite{aftersight} and DoublePlay~\cite{doubleplay}.
%\parikshan provides a systematic framework for decoupling execution tracing from production service.

\subsubsection{CaseStudy: Execution Tracing}

We now look into MySQL bug\#15811 (see also section~\ref{sec:parikshanCasestudy}), this is a performance bug which happens when dealing with complex scripts (Japanese, Chinese etc.). Let us look at how a debugger would go about finding the root cause of such a bug.
Firstly, let us say that a high level report of the bug is provided by the user of a deployed production server.  
The report states that a certain category of queries are having higher than expected transaction latencies.
The user reports this as a potential bug and asks for it to be investigated.
Based on the user report, a post-facto MySQL replica is created for debugging and analysis by the developer/debugger.
The debugger then uses ~\emph{SystemTap} tracing tool~\cite{systemtap} to trace the execution of the application. 
This instrumentation is optionally triggered whenever the input queries are found to be ``chinese". 
This can be easily done in MySQL by setting trigger points when the language specification in the query is read inside MySQL query parser.

Since the bug reported is a performance bug, the developer must first find out which module and specifically which function is the cause of the bug.
To find the time taken in each function, function-level begin and exit instrumentation is added and the timestamp for each function is collected as log evidence.
This detailed evidence allows the debugger to localize and find the root-cause of the error inside the \emph{``my\_strcasecmp()"} function in comparison to the time taken by the function for latin based queries.
Once the performance bug, has been localized.
The debug-container can then be dis-connected from the proxy (alternatively proxy input forwarding can be stopped).
Now, some of the ``read-only" queries which triggered this bug can be re-sent to the MySQL database, and a step-by-step execution can be followed inside this function using deeper instrumentation to further understand the code execution.

In our experiments for localizing this bug, we found that function level instrumentation for profiling time-spent in each function can take from 1.5x to 1.8x overhead.
This is clearly un-acceptable in production systems.
However, the \parikshan framework allows for capturing such execution traces without impacting user-facing performance of the MySQL database.
While such persistent bugs can be debugged offline, it may be argued that such bugs can also be debugged in an offline debugging environment.
However, given no previous knowledge \parikshan gives a valuable ``first-attack'' mechanism to debug unknown problems.

\subsection{Statistical Debugging}
\label{sec:activeStatisticalDebugging}

Statistical debugging aims to automate the process of isolating bugs by profiling several runs of the program and using statistical analysis to pinpoint the likely causes of failure. 
The seminal work on statistical debugging~\cite{statisticalDebugging}, has lead to several advanced approaches~\cite{holmes,adaptive,statisticalPerformance}, and is now a well established debugging methedology. 

The core mechanism of statistical debugging is to have probabilistic profiling, by sampling execution points and comparing the execution traces for failed and successful transactions.
It then uses statistical models to identify path profiles that are strongly predictive of failure. 
This can be used to iteratively localize the bug causing execution, and can then be manually analyzed by \parikshan.

Statistical debugging relies on the sampling frequency of the instrumentation, which can be decreased to reduce the instrumentation overhead.
However, the instrumentation frequency needs to be statistically significant for such testing to be successful. 
Unfortunately, overhead concerns in the production environment can limit the frequency of statistical instrumentation.
In \parikshan, the buffer utilization can be used to control the frequency of such statistical instrumentation in the debug-container. 
This would allow the user to utilize the slack available in the debug-container for instrumentation to it's maximum, without leading to an overflow. 
Thereby improving the efficiency of statistical testing.

Statistical debugging is one of the systematic bug localization approaches that can be directly applied in the \debugcontainer, with the added advantage that the amount of instrumentation that can be applied in the debug-container is much higher than production containers. 
Apart from regular semantic bugs, previous body of works have shown that statistical debugging is useful in detecting a variety of other bugs like concurrency bugs~\cite{statisticalConcurrency}, and performance~\cite{statisticalPerformance}.

\subsection{Staging Record and Replay}
\label{sec:activeStagedRecordReplay}

\begin{figure}[h!]
	
	\centering
	\includegraphics[width=0.99\textwidth]{guided/figs/stagedRecordReplay.pdf}
	\caption{Staged Record and Replay using \parikshan}
	\label{fig:stagedRecordReplay}
\end{figure}

One well known sub-category of debugging service-oriented applications are record-replay infrastructures.
In the past decade there have been numerous record-and-replay infrastructures~\cite{pres,friday,jockey,mugshot,revirt,r2,laadan2010transparent,mutableReplay} which have been introduced in academia.
The core focus of these techniques is to faithfully reproduce the execution trace and allow for offline debugging.
However, in order to faithfully reproduce the exact same instrumentation, the recording phase must record a higher granularity of execution.
Unfortunately, this means a higher overhead at the time of recording in the production system.
Such recording overhead is usually unacceptable in most production systems.

\begin{comment}
The PRES~\cite{pres} record-replay tool for instance compares recording at different granularity levels from the basic (system calls, input signals, thread scheduling), to synchronization points, and finally to basic block and shared read-write level.
Higher granularity level led to throughput degradation in all server applications (from 7.05\% for base in Apache to 76\% for R-W level in OpenLDAP).
Replay of logs recorded at the basic level were often unable to trigger the bug, while higher level of recording granularity increased the chances of triggering the bug as well as the number of retries required to trigger the bug in replay. 
This indicates the importance of higher granularity recording for understanding and triggering bugs in replay. 	
\end{comment}

Record and replay can be coupled with the \debugcontainer to avoid any overhead on the \productioncontainer.
This is done by staging the recording for record-and-replay in the \debugcontainer instead of the production, and then replaying that for offline analysis.
In figure~\ref{fig:stagedRecordReplay} we show how the production system can first be ``live-cloned". A copy of the container's image can be stored/retained for future offline replay - this incurs no extra overhead as taking a live snapshot is a part of the live-cloning process. Recording can then be started on the \debugcontainer, and logs collected here can be used to do offline replay.

We propose that \parikshan provides a viable alternative to traditional record-replay techniques, whereby a high granularity recording can be done on the \debugcontainer instead of the \productioncontainer.
The amount of recording granularity ( or the amount of recording overhead) will depend on the workload of the system, and how much idle time to the \debugcontainer has to catch up to the production container. 
Admittedly, \textbf{non-determinism} can lead to different execution flows in the \debugcontainer v.s. the \productioncontainer (with low probability as the system is a clone of the original).
Hence simply replaying an execution trace in the \debugcontainer, may not lead to the same execution which triggers the bug.
However several existing record-and-replay techniques offer search capabilities to replay and search through all possible concurrent schedules which could have triggered a non-deterministic error~\cite{dpor,best}.
%We propose that the exact high granularity recording of the using the \debugcontainer for recording instead of the \productioncontainer is a viable alternative to traditional record-replay techniques.


\subsubsection{CaseStudy: Staged Record-Replay}

To show a use-case for staged record-replay we look at Redis bug \#761 (see also section~\ref{sec:parikshanCasestudy}). 
As explained earlier this Redis bug is an integer overflow error.
Let us look at how a debugger would go about finding the root cause of such a bug.
Imagine that we are doing staged record-replay, whereby the debug container is getting duplicated network inputs, and the \debugcontainer is ``recording" the execution in parallel by activating commodity record-replay tools.

The bug happens over a period of time when a request happens for an addition/storage of a large integer, which leads to an integer overflow error.
The execution trace of this bug will be captured in the ``record" log of the debug container, and can be replayed offline for debugging purposes.
Since the bug is a crashing bug, the execution can be replayed and a debugger can be attached to the execution in the replay mode.
Ideally, the transaction which has caused the error, is the last network input. 
This transaction can be executed step-by-step with roll-back to localize the error point.

Under normal conditions, this recording would have caused an overhead on the production container. 
\parikshan decouples it's staged recording and can proceed without any overhead to the production system.
Recording overhead differs for different tools, and is often impractical for production software. 
However, by decoupling recording instrumentation from the production container, we can record at high granularities all the time, and replay whenever a bug is observed.

\subsection{A-B Testing}

\begin{figure}[h!]
	
	\centering
	\includegraphics[width=0.99\textwidth]{guided/figs/ABTesting.pdf}
	\caption{Traditional A-B Testing}
	\label{fig:abTesting}
\end{figure}

A/B testing (sometimes called split testing) is comparing two versions of a web page to see which one performs better. 
You compare two web pages by showing the two variants (let's call them A and B) to similar visitors at the same time.
User operations in A can then be compared to user scenario's in B to understand which is better, and how well it was received.
Typically A/B testing is done to test and verify beta releases and optimizations, and how they impact the user.
A/B Testing can be extended in \parikshan by leveraging the \debugcontainer  for evaluating patches for performance optimization or functional improvements. 
These patches must be functionally similar and have same network level input/output to ensure forward progress.
\parikshan can thereby provide limited insight into beta releases before they are introduced in production. 


\subsection{Interactive Debugging}

The most common debugging tools used in the development environment are interactive debuggers. 
Debugging tools like gdb~\cite{gdb}, pdb, or eclipse~\cite{eclipse}, provide intelligent debugging options for doing interactive debugging. 
This includes adding breakpoints, watch-points, stack-unrolling etc.
The downside to all of these techniques is that not only do they incur a performance overhead, they need to stop the service or execution to allow interactive debugging.
Once a process has been attached to a debugger, a shadow process is also attached to it and the rest of the execution follows with just-in-time execution, allowing the debugger to monitor the progress step-by-step therefore making it substantially easier to catch the error.
Clearly, such techniques are meant for development environment and cannot be applied to production environments.

However this can be easily applied towards the \debugcontainer, where the execution trace can be observed once a breakpoint has been reached.
While this does mean that the replica will not be able to service any more requests (except for those that have been buffered), the request which is inside the breakpoint will be processed.
Generally breakpoint and step-by-step execution monitoring is used for a limited scope of execution within a single transaction.
Once, finished future transactions can also be debugged after doing a re-sync by applying live cloning again.

\subsubsection{CaseStudy: Interactive Debugging}

As mentioned earlier, the downside of interactive debugging is that it puts significant overhead on the running program.
This is because debuggers can do step-by-step execution on breakpoints, and can exactly map the execution of given training requests.
Let us look at a memory leak example from Redis bug \#417. 
The bug shows itself as an increasing memory footprint ( or a leak), which can be easily observed from any monitoring software by looking at the amount of memory being consumed by the Redis server.
Once the bug is reported, the developer can trigger a live-clone in \parikshan and create a \debugcontainer.

Since this bug is a slow-increasing memory leak it does not lead to an imminent crash (crash could take a few days). 
Monitoring software and periodic memory or file process snapshots in the debug-container (use of lsof command, or vsz) can tell us that stale connections are left from the master to the slave.
This indicates to the debugger that the problem is likely in replication logic of master.
We then put a breakpoint in the replication source code at the point when a connection is created.
This ``breakpoint'' will be triggered whenever a replication is triggered (replication is triggered periodically), and will allow the debugger to step-by-step execute the process.
\parikshan debug-containers can manage significant overhead before they diverge from the production container. 
However, once step-by-step execution is triggered it is ideally going to allow the debugger access to only those transactions currently in the proxy buffer (depending on the application, transactions and buffer-size, the buffer could have several transactions). 
The step-by-step execution, will give a detailed understanding to the user of the transaction that is being currently executed, and can also be used for those which are in the buffer.
The step-by-step inspection will allow the debugger to see that the connection was not closed at the end of the periodic replication process (which is causing the leak).
The error was caused because of a condition which was skipping the ``connection close'' logic when db was configured \textgreater=10.


\subsection{Fault Tolerance Testing}

One of the places that \parikshan can be applied is for fault tolerance testing.
To motivate this let us look at Netflix's current testing model.
Netflix has a suite of real-time techniques~\cite{chaosengineering} for testing fault-tolerance of it's systems.
Amongst them, chief is chaos monkey~\cite{chaosmonkey}, which uses fault injection in real production systems to do fault tolerance testing.
It randomly injects time-outs, resource hogs etc. in production systems. 
This allows Netflix to test the robustness of their system at scale, and avoid large-scale system crashes. 
The motivation behind this approach is that it's nearly impossible to create a large-size test-bed to have a realistic fault tolerance testing for the scale of machines that Netflix has. 
Chaos Monkey allows Netflix to do it's fault tolerance testing at a small cost to the customer experience, while avoiding fatal crashes which could lead to longer downtimes.
The obvious downside of this approach is that the service becomes temporarily unavailable and re-sets, or forces a slow-down on the end-user experience (this may or may not be visible to the user). 

Since \parikshan can be run in a live system, it can be attached to a scaled out large-system, and can allow users to test for faults in an isolated environment, by creating a sub-set of \debugcontainer container nodes where the fault will be injected.
The only limitation being that the fault-injections should be such that the impact of these faults can be isolated to the targeted \debugcontainer systems, or a sub-domain of a network which has been cloned, and the tolerance built into the system can be tested (it would be too expensive to clone the entire deployment).
This allows for fault tolerance testing, and at the same time hiding it's impact from the end-user.

\section{Budget Limited, Adaptive Instrumentation}
\label{sec:activeBudgetLimited}

As explained in section~\ref{sec:parikshanDesign}, the asynchronous packet forwarding in our network duplication results in a \debugwindow.
The \debugwindow is the time before the buffer of the debug-container overflows because of the input from the user.
The TCP connection from end-users to production-containers are synchronized by default.
This means that the rate of incoming packets is limited by the amount of packets that can be processed by the production container.
On the other hand, packets are forwarded asynchronously to an internal-buffer in the debug-container.
The duration of the \debugwindow is dependent on the incoming workload, the size of the buffer, and the overhead/slowdown caused due to instrumentation in the debug-container.
Each time the buffer is filled, requests are dropped, and the debug-container can get out of sync with the production container.
To get the debug-container back in sync, the container needs to be re-cloned.
While duplicating the requests has negligible impact on the production container, cloning the production container can incur a small suspend time(workload dependent).
\iffalse
\begin{wrapfigure}{R}{0.75\textwidth}
	\centering
	\includegraphics[width=0.45\textwidth]{queue/figs/queue.pdf}
	\caption{\label{fig:queueModel}An example of a simple queue applied to a SOA application. Here the arrival rate of the requests to the queue is a poisson process with rate $\lambda$, and the service time of each request is a poisson process with rate $\mu$. In the default SOA settings, the client request is being sent into a blocking TCP queue, where the incoming requests are rate limited by $\mu$. Hence, there is never any buffer overflow}
\end{wrapfigure}
\fi

The duration of the \debugwindow can be increased by reducing the instrumentation.
At the same time we wish to increase the maximum information that can be gained out of the instrumentation to do an effective bug diagnosis.
Essentially for a given buffer size and workload, there is a trade-off between the information gain due to more instrumentation and the duration of the \debugwindow.
Hence our general objective is to increase the information gain through instrumentation while avoiding a buffer overflow.

We divide this task into  pro-active and re-active approaches which can complement each other. Firstly, we pro-actively assign budgets using queuing theory. 
Using a poisson distribution for average processing time of each request and the inter-arrival time of requests, we can find expected buffer sizes for a reasonable debug-window length. 
Secondly, we propose a reactive mechanism, whereby buffer utilization can be continuously monitored and the instrumentation sampling can be exponentially reduced if the buffer is near capacity. 
%This can be combined with statistical debugging to have the maximum information gain possible. 

\subsection{Proactive: Modeling Budgets}
\label{sec:activeProactiveModeling}


\begin{figure}[t!]

		\centering
		\includegraphics[width=0.7\textwidth]{queue/figs/queue.pdf}
		\caption{\parikshan applied to a mid-tier service}
		\label{fig:queueModel}
\end{figure}
\begin{figure}
		\centering
		\includegraphics[width=0.99\textwidth]{queue/figs/queueCloned.pdf}
		\caption{External and Internal Mode for live cloning: P1 is the production, and D1 is the debug container.}
		\label{fig:queueClonedModel}
\end{figure}
	
%\caption{Modeling \livedebugging using queuing theory}
%\end{figure*}

\iffalse
\begin{figure}[h]
	\begin{center}
		\includegraphics[width=0.8\textwidth]{queue/figs/queue.pdf}
		\caption{An example of a simple queue applied to a SOA application. Here the arrival rate of the requests to the queue is a poisson process with rate $\lambda$, and the service time of each request is a poisson process with rate $\mu$. }
		%\caption{\parikshan applied to a mid-tier service: It is comprised of: (1) Clone Manager for Live Cloning, (2) Proxy Duplicator to duplicate network traffic, and (3) Proxy Aggregator to replay network traffic to the cloned debug container.}
		\label{fig:queueModel}
	\end{center}
\end{figure}
\fi
 
In this section we model the testing window by using concepts well used in queuing theory (for the sake of brevity we will avoid going into too much detail, readers can find more about queuing theory models~\cite{queueBook}).
Queueing theory is commonly used to do capacity planning for service-oriented architectures(SOA).
Queues in a SOA application can be modeled as a M/M/1/K queue (Kendall's notation~\cite{kendall}).
Kendall's notation is a well known model which allows a compact representation for queues in SOA architectures.
This is a shorthand notation of the type A/B/C/D/E where A, B, C, D, E describe the queue.
%This notation may be easily applied to cover a large number of simple queueing scenarios.
The standard meanings associated with each of these letters are summarized below.\\

\begin{framed}
	\noindent \textbf{A} represents the \emph{inter-arrival time distribution}\\
	\textbf{B} represents the \emph{service time distribution}\\
	\textbf{C} gives the \emph{number of servers} in the queue\\
	\textbf{D} gives the \emph{maximum number of jobs that can be there in the queue}.\\
	%If this is not given then the default value of infinity \infinity is assumed implying that the queue has an infinite number of waiting positions\\
	\textbf{E} represents the Queueing Discipline that is followed. The typical ones are First Come First Served (FCFS), Last Come First Served (LCFS), Service in Random Order (SIRO) etc. If this is not given then the default queueing discipline of FCFS is assumed.
\end{framed}

The different possible distributions for \textbf{A} and \textbf{B} above are:

\begin{framed}
	\noindent \textbf{M} exponential distribution\\
	\textbf{D} deterministic distribution\\
	\textbf{E$_{\text{k}}$} Erlangian (order k)\\
	\textbf{G} General
\end{framed}



Figure \ref{fig:queueModel} represents a simple client-server TCP queue in an SOA architecture based on the M/M/1/K queue model.
An M/M/1/K queue, denotes a queue where requests arrive according to a poisson process with rate $\lambda$, that is the inter-arrival times are independent, exponentially distributed random variables with parameter $\lambda$ .
The service times are also assumed to be independent and exponentially distributed with parameter $\mu$. 
Furthermore, all the involved random variables are supposed to be independent of each other.
In the case of a blocking TCP queue common in most client-server models, the incoming request rate from the client is throttled based on the request processing time of the server. 
This ensures that there is no buffer-overflows in the system.

\iffalse
\begin{figure}[h]
	\begin{center}
		\includegraphics[width=0.8\textwidth]{queue/figs/queueCloned.pdf}
		\caption{This figure shows how the simple queueing model can be extended to \parikshan. Here instead of looking at the TCP buffer, we look at the packet arrival and processing time for the proxy duplicator.}
		%\caption{\parikshan applied to a mid-tier service: It is comprised of: (1) Clone Manager for Live Cloning, (2) Proxy Duplicator to duplicate network traffic, and (3) Proxy Aggregator to replay network traffic to the cloned debug container.}
		\label{fig:queueClonedModel}
	\end{center}
\end{figure}
\fi

In \parikshan, this model can be extended to a cloned model as shown in figure \ref{fig:queueClonedModel}.
The packets to both the production and the debug cloned containers are routed through a proxy which has internal buffer to account for slowdowns in request processing in the debug container. 
Here instead of the TCP buffer, we focus on the request arrival and departure rate to and from the proxy duplicators buffer.
The incoming rate remains the same as $\lambda$, as the requests are asynchronously forwarded to both containers without any slowdown. 
%The request processing rate of the debug container is $\mathrm{\mu_{2}}$, where $\mathrm{\mu_{2}}$ $>$ $\mathrm{\mu_{1r}}$ (the processing time for the production container) depending on the overhead due to instrumentation in the debug-container.

To simplify the problem, we identify the following variables:

This is the maximum capacity at which the \productioncontainer can process requests

\begin{equation}\label{eq:1}
\mu_{1} = \emph{processing time for requests of original container}
\end{equation}

This is the maximum capacity at which the \debugcontainer can process requests

\begin{equation}\label{eq:2}
\mu_{2} = \emph{processing time for requests of debug container}
\end{equation}

Taking the above two equations, the overhead can be modeled as follows

\begin{equation}\label{eq:3}
\mu_{3} = \mu_{1} - \mu_{2}  = \emph{slowdown of debug compared to original}
\end{equation}

The remaining processing time for both the production container and the debug container is going to be the same. 
Since the TCP buffer in the production container is a blocking queue, we can assume that any buffer overflows in the proxy buffer are only caused because of the instrumentation overhead in the debug-container, which is accounted for by $\mu_{3}$.

However for our debug system to be stable the goal still remains to allocate debugging overhead such that:

\begin{equation}\label{eq:four}
  \lambda < \mu_{2}
\end{equation}

The equation above gives us the basis to build certain guidlines for the instrumentation overhead guarantees in the debug containers, and how to create the buffer. 
As can be easily understood, that if the rate of incoming requests ($\lambda$) to the \productioncontainer itself is continuously equal to $\mu_{1}$ (i.e. it's maximum capacity), then intuitively there is no ``slack" available to \debugcontainer to catch up to the \productioncontainer. 
However for production services, which generally run far below the maximum capacity, there will be significant opportunity for instrumentation in the \debugcontainer, without impacting the user performance.
This \debugcontainer uses the idle time in between requests to catch up to the \productioncontainer, thereby remaining in synch.

The advantage of the of our internal buffer in the duplication proxy in this scenario, is that it provides a lengthy \debugwindow in the case of a spike in the workload.
Once the \debugcontainer starts lagging behind the \productioncontainer, the requests start piling in the internal buffer. 
Spikes are generally short, bursty and infrequent, hence given some idea of the spike in the workload the operator can set the buffer size and instrumentation such that he can avoid the overflow.


\subsection{Extended Load-balanced duplicate clones}

Our model can be further extended into a load-balanced instrumentation model as shown in figure~\ref{fig:queueBalanced}. 
This is useful when the debugging needs to be higher, but we have a lower overhead bound through only one clone.
Here we can balance the instrumentation across more than one clones, each of which receive the same input.
They can together contribute towards debugging the error, as well as increase the amount of instrumentation that can be done without incurring an overhead.
Hence, if earlier we had enough slack in the ``production system" to have a 2x overhead instrumentation in the \debugcontainer, with an extra replica, the amount instrumentation can be potentially raised to 4x overhead.

\begin{figure}[h!]
	\begin{center}
		\includegraphics[width=0.8\textwidth]{queue/figs/queueBalanced.pdf}
		\caption{This figure shows how queuing theory can be extended to a load balanced debugging scenario. Here each of the debug container receive the requests at rate $\lambda$, and the total instrumentation is balanced across multiple debug containers.}
		\label{fig:queueBalanced}
	\end{center}
\end{figure}

%TODO - Finish simulation experiments if time permits
\begin{comment}
\subsection{Simulation}
\label{sec:queueEval}

This section is a \textbf{place-holder} and will be extended in a later iteration of the thesis

In this section we present our evaluation strategy for simulating various workloads to model the hitting time and find how much resources to be allotted/instrumentation can be done for a given workload. 
Our goal of these experiments is to provide the user with an upper-bound for instrumentation in terms of percentage of the average transaction time, based on a given buffer size, and expected time-window needed for debugging.	
\end{comment}
%In our experiments, we plot the time-window observed based on the hitting time explained in the equation. 
%For a given buffer size, we run multiple iterations of instrumentation until we hit the pre-defined time limit. 
%In this case we assume that the time-taken to capture a trace good enough for debugging the problem, is 10 mins. 
%Please note, this is not the total debugging time, but rather just a trace instance large enough to capture the systems and root-cause.
%The time window will depend on the use-case, and is a user-specified argument.


\begin{comment}

The utilization factor for an M/M/1/K queue can be formulated as follows:

\begin{equation}
\rho = \frac{\lambda}{\mu}
\end{equation}
\\ \\

Now based on this notation and the expected number of requests buffered in the queue the blocking probability for the queue can be calculated as follows:

\begin{equation}
P_{b} = \frac{(1-\rho)\rho^{K}}{1-\rho^{K+1}}
\end{equation}
\\ \\

The expected time for the queue to fill up:

\begin{equation}
E[] =
\end{equation}

Given the expected time for processing each request in this model

\begin{equation}
E[] =
\end{equation}

The overhead bounds for instrumentation in each request can be modeled as follows:

\begin{equation}
E[] = 
\end{equation}

This gives us the following relationship between overhead and expected time for the queue to fill up.

\begin{equation}
E[] = relationship\ with\ request\ overhead
\end{equation}


Next we extend this to our asynchronous load-balanced model, the requests can be forwarded in multiple debug containers.
This can reduce the waiting time further, for the asynchronous model the equations can be extended as follows\\ \\
\end{comment}


\subsection{Reactive: Adaptive Instrumentation}
\label{sec:activeAdaptiveInstrumentation}

Adaptive instrumentation reduces or increases sampling rate of the dynamic instrumentation in order to decrease the overhead. 
This allows the debug-container time to catch up to the production container without causing a buffer overflow.

\begin{figure*}[ht!]
	\begin{center}
		\includegraphics[width=0.96\textwidth]{queue/figs/reactive-controller.png}
		\caption{Reactive Instrumentation}
		\label{fig:reactive}
	\end{center}
\end{figure*}

A mechanism similar to TCP's network congestion avoidance mechanisms can be applied on the monitoring buffer.
We also derive inspiration from statistical debugging~\cite{statisticalPerformance,holmes,statisticalDebugging}, which shows how probabilistically instrumenting \emph{predicates}, can assist in localizing and isolating the bug. 
Predicates can be branch conditionals, loops, function calls, return instructions and if conditionals.
Predicates provide significant advantages in terms of memory/performance overheads.
Instead of printing predicates, they are usually counted, and a profile is generated.
This reduces the amount of instrumentation overhead, and several predicates can easily be encoded in a small memory space.
Similar techniques have also been applied for probabilistic call context encoding in-order to capture execution profiles with low overhead.


The sampling rate of instrumentation in the debug-container can be modified based on the amount of buffer usage.
There are three key components of our adaptive instrumentation mechanism.

\begin{itemize}
	\item \textbf{Monitoring Buffer:}
	The first step involves monitoring the buffer usage of the network duplicator. 
	If the buffer usage is more than X percentage of the buffer, the sampling rate of instrumentation can be exponentially decreased.
	This would increase the idle time in the debug container allowing it to catch up to the production and reducing the buffer usage.
	
	\item \textbf{Controller:}
	The controller allows debuggers to control the sampling rate of instrumentation.
	The sampling rate can be controlled for each predicate.
	Similar to statistical debugging the predicates with lower frequency can have higher sampling rates, and predicates with higher frequency can have lower sampling rates. This ensures overall better information gain in any profile collected.  	
	
\end{itemize}

\subsection{Automated Reactive Scores}

A statistical record is maintained for each predicate, and the overall success of execution is captured by the error log.
We assume worker-thread model, where we are able to associate the success/failure of the transaction by associating process-ids and error log transaction ids.
The instrumentation cost for each instrumentation profile can be as follows.

\begin{equation}
\sum\limits_{i=1}^{i=n} x_i  = InstrumentationScore(x)*StatisticalScore(x)
\end{equation}

Each predicate is given a total score based on the following parameters:

\begin{itemize}
	\item \textbf{Statistical Importance Score:} The statistical importance score defines the importance of each predicate as an indicator for isolating the bug.The main idea is derived from statistical debugging work done by Liblit et Al
	\item \textbf{Instrumentation Overhead Score:} Adaptive score keeping track of counters of each predicate. Can be used as a weighing mechanism for figuring out the total cost.
\end{itemize}


\begin{comment}
\subsection{Feasibility}
\label{sec:queue_feasability}
\xxx{Re-label and figure out what to write here}

%In ~\cite{parikshanTR,parikshanQueue} we looked into the model generation and some early theoretical experiments.
We also created a simulator in C/C++ which can buffer size, service times, overhead and incoming request rate to emulate the queuing model.
The tool shows buffer overflow, and time taken to reach the overflows.

Our initial investigation has shown that the debug container can sustain significant spikes in overhead without overflowing the production container, as long as the production container is under-capacity. 
In particular we saw a linear increase in the buffer usage for the debug-container once the workload to the production container reaches it's maximum threshold capacity.
This concurred with our earlier assumptions, as a sustained spike in the production container (such that the spike is more than the maximum threshold), would not allow the debug container to catch up.
We verified these observations both in our \parikshan prototype, as well as the simulation results shown in Appendix~\ref{appendix:simulation}.
One of the tests in our optimized linux pipe based buffer network proxy, made us realize that for smaller budgets, it is difficult to observe the increase in the buffer usage before a spike leads to an overflow.
While the simulation gives us fine grained control in our observations, our micro-evaluation on some real-world software made us realize that control in reality can be much more coarse-grained.

We are currently in the process of making an adaptive sampling instrumentation mechanism. 
For this we plan to use tools like \iprobe, systemtap~\cite{systemtap} and dyninst~\cite{dyninst}. These have been previously used in several large-scale instrumentation projects and demonstrated to be effective with low overheads. We will also derive inspiration for our model from previous statistical debugging approaches~\cite{statisticalPerformance}. These have been demonstrated to be effective in resolving several real-world bugs. This will be explored as part of the thesis.

\end{comment}

\section{Summary}
\label{sec:activeSummary}

In this section we have discussed how \parikshan can be applied in real-world bugs and how a developer can actually do debugging of production systems. 
To explain the process better we first categorized the debugging scenarios into two distinct categories: \emph{post-facto}, and \emph{proactive} debugging.
We have then described several existing debugging tools which can be applied in \parikshan's debug-container to make debugging more efficient and effective.
Lastly, we introduced a budget limited adaptive debugging technique which can be used to model ``allowed" instrumentation overhead for continuous debugging in the \debugcontainer.
